{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Dropout, TextVectorization\n    \n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-19T23:33:01.771559Z","iopub.execute_input":"2022-07-19T23:33:01.771940Z","iopub.status.idle":"2022-07-19T23:33:01.782346Z","shell.execute_reply.started":"2022-07-19T23:33:01.771914Z","shell.execute_reply":"2022-07-19T23:33:01.780184Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv',index_col = 0, on_bad_lines = \"skip\")\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv',index_col = 0, on_bad_lines = \"skip\")\ntrain_data.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T23:33:07.649628Z","iopub.execute_input":"2022-07-19T23:33:07.650020Z","iopub.status.idle":"2022-07-19T23:33:07.705902Z","shell.execute_reply.started":"2022-07-19T23:33:07.649991Z","shell.execute_reply":"2022-07-19T23:33:07.704844Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   keyword location                                               text  target\nid                                                                            \n1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1\n8      NaN      NaN  #RockyFire Update => California Hwy. 20 closed...       1\n10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...       1\n13     NaN      NaN  I'm on top of the hill and I can see a fire in...       1\n14     NaN      NaN  There's an emergency evacuation happening now ...       1\n15     NaN      NaN  I'm afraid that the tornado is coming to our a...       1\n16     NaN      NaN        Three people died from the heat wave so far       1\n17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...       1\n18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...       1\n19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago       1\n20     NaN      NaN  Damage to school bus on 80 in multi car crash ...       1\n23     NaN      NaN                                     What's up man?       0\n24     NaN      NaN                                      I love fruits       0\n25     NaN      NaN                                   Summer is lovely       0\n26     NaN      NaN                                  My car is so fast       0\n28     NaN      NaN                       What a goooooooaaaaaal!!!!!!       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Three people died from the heat wave so far</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Damage to school bus on 80 in multi car crash ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What's up man?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I love fruits</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Summer is lovely</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>My car is so fast</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a goooooooaaaaaal!!!!!!</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.head(20)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T23:33:10.374991Z","iopub.execute_input":"2022-07-19T23:33:10.375552Z","iopub.status.idle":"2022-07-19T23:33:10.390735Z","shell.execute_reply.started":"2022-07-19T23:33:10.375503Z","shell.execute_reply":"2022-07-19T23:33:10.389764Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   keyword                        location  \\\nid                                           \n0      NaN                             NaN   \n2      NaN                             NaN   \n3      NaN                             NaN   \n9      NaN                             NaN   \n11     NaN                             NaN   \n12     NaN                             NaN   \n21     NaN                             NaN   \n22     NaN                             NaN   \n27     NaN                             NaN   \n29     NaN                             NaN   \n30     NaN                             NaN   \n35     NaN                             NaN   \n42     NaN                             NaN   \n43     NaN                             NaN   \n45     NaN                             NaN   \n46  ablaze                          London   \n47  ablaze  Niall's place | SAF 12 SQUAD |   \n51  ablaze                         NIGERIA   \n58  ablaze                  Live On Webcam   \n60  ablaze        Los Angeles, Califnordia   \n\n                                                 text  \nid                                                     \n0                  Just happened a terrible car crash  \n2   Heard about #earthquake is different cities, s...  \n3   there is a forest fire at spot pond, geese are...  \n9            Apocalypse lighting. #Spokane #wildfires  \n11      Typhoon Soudelor kills 28 in China and Taiwan  \n12                 We're shaking...It's an earthquake  \n21  They'd probably still show more life than Arse...  \n22                                  Hey! How are you?  \n27                                   What a nice hat?  \n29                                          Fuck off!  \n30                              No I don't like cold!  \n35                         NOOOOOOOOO! Don't do that!  \n42                             No don't tell me that!  \n43                                          What if?!  \n45                                           Awesome!  \n46  Birmingham Wholesale Market is ablaze BBC News...  \n47  @sunkxssedharry will you wear shorts for race ...  \n51  #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...  \n58  Check these out: http://t.co/rOI2NSmEJJ http:/...  \n60  PSA: IÛªm splitting my personalities.\\n\\n?? t...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>We're shaking...It's an earthquake</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>They'd probably still show more life than Arse...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Hey! How are you?</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a nice hat?</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fuck off!</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No I don't like cold!</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOOOOOOOOO! Don't do that!</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No don't tell me that!</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What if?!</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Awesome!</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>ablaze</td>\n      <td>London</td>\n      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>ablaze</td>\n      <td>Niall's place | SAF 12 SQUAD |</td>\n      <td>@sunkxssedharry will you wear shorts for race ...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>ablaze</td>\n      <td>NIGERIA</td>\n      <td>#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>ablaze</td>\n      <td>Live On Webcam</td>\n      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>ablaze</td>\n      <td>Los Angeles, Califnordia</td>\n      <td>PSA: IÛªm splitting my personalities.\\n\\n?? t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_x = train_data.iloc[:,:-1]\ntrain_y = train_data.iloc[:, -1]\ntrain_x = train_x.drop(columns=['keyword', 'location'])\nprint(train_x.iloc[1])\nprint(train_x.shape)\nprint(train_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T00:05:07.577317Z","iopub.execute_input":"2022-07-20T00:05:07.577704Z","iopub.status.idle":"2022-07-20T00:05:07.586858Z","shell.execute_reply.started":"2022-07-20T00:05:07.577675Z","shell.execute_reply":"2022-07-20T00:05:07.586154Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"text    Forest fire near La Ronge Sask. Canada\nName: 4, dtype: object\n(7613, 1)\n(7613,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-07-19T23:50:49.666966Z","iopub.execute_input":"2022-07-19T23:50:49.667352Z","iopub.status.idle":"2022-07-19T23:50:49.672323Z","shell.execute_reply.started":"2022-07-19T23:50:49.667328Z","shell.execute_reply":"2022-07-19T23:50:49.671011Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_x, train_y, val_x, val_y = train_test_split(train_x, train_y, test_size=0.33, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T23:50:57.619902Z","iopub.execute_input":"2022-07-19T23:50:57.620238Z","iopub.status.idle":"2022-07-19T23:50:57.628922Z","shell.execute_reply.started":"2022-07-19T23:50:57.620213Z","shell.execute_reply":"2022-07-19T23:50:57.628002Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# the size of the vocabulary we'll use\nvocab_size = 12000\nmaxlen = 150\n\ndef load_preprocess_imdb(num_words=vocab_size, prop_vec=prop_vec, maxlen=maxlen, vectorize=False):\n    \n    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n    train, testval = imdb.load_data(num_words=num_words, maxlen=maxlen, oov_char=0)\n    np.warnings.filterwarnings('default', category=np.VisibleDeprecationWarning)   \n\n    ### Process the data\n    ### Merge train and testval, but then split again into train, test, val sets (according to prop_vec). You can use utils.train_test_val_split().)\n    ### - If vectorize=True, then you must encode the features of each example into vectors of vocab_size entries\n    ### such that entry i contains the number of time word i appeared in the sequence\n    ### - If vectorize=False, then you must encode the features of each examples as a sequence of size maxlen (represented as a np.array()).\n    ### Make sure to pad sequences with 0 as appropriate.\n    ###* put your code here (~10-15 lines) *###\n    from keras.preprocessing.sequence import pad_sequences\n    \n    train_x, train_y = train\n    test_x, test_y = testval\n    \n    all_x = np.concatenate([train_x, test_x])\n    all_y = np.concatenate([train_y, test_y])\n    \n    \n    if vectorize:\n        tokenizer = keras.preprocessing.text.Tokenizer(num_words = vocab_size, lower=False, char_level=True)\n        train_x = tokenizer.sequences_to_matrix(train_x, mode = 'count')\n        test_x = tokenizer.sequences_to_matrix(test_x, mode = 'count')\n                                                       \n    else:\n        all_x = pad_sequences(all_x, maxlen=maxlen)\n\n    train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, prop_vec)\n\n    return train_x, train_y, test_x, test_y, val_x, val_y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_compile_rnn(input_shape=[None], vocab_size=vocab_size, embedding_size=128, num_outputs=1, verbose=False): \n    \n    from tensorflow.keras.layers import Embedding, GRU, LSTM\n    \n    max_len = 4  # Sequence length to pad the outputs to.\n    \n    model = keras.models.Sequential(name='twitter-RNN')\n    \n    vectorize_layer = tf.keras.layers.TextVectorization(\n                                            max_tokens=vocab_size,\n                                            output_mode='int',\n                                            output_sequence_length=max_len)\n    \n    #vectorize_layer.adapt(text_dataset.batch(64))\n    \n    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n    model.add(vectorize_layer)\n    \n    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_size))\n    \n    model.add(GRU(64, return_sequences=True, name='gru1'))\n    model.add(GRU(32, return_sequences=True, name='gru2'))\n    model.add(GRU(24, return_sequences=True, name='gru3'))\n    model.add(GRU(8, name='gru4'))\n    \n    model.add(Dense(num_outputs, activation='sigmoid', name='output'))\n\n    if verbose:\n        model.summary()\n        \n    opt = keras.optimizers.Adam(lr=0.001)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-20T00:15:32.637178Z","iopub.execute_input":"2022-07-20T00:15:32.637503Z","iopub.status.idle":"2022-07-20T00:15:32.651838Z","shell.execute_reply.started":"2022-07-20T00:15:32.637479Z","shell.execute_reply":"2022-07-20T00:15:32.650651Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = create_compile_rnn(verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T00:15:35.225709Z","iopub.execute_input":"2022-07-20T00:15:35.226066Z","iopub.status.idle":"2022-07-20T00:15:36.061001Z","shell.execute_reply.started":"2022-07-20T00:15:35.226042Z","shell.execute_reply":"2022-07-20T00:15:36.059546Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Model: \"twitter-RNN\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization_7 (TextVe (None, 4)                 0         \n_________________________________________________________________\nembedding_4 (Embedding)      (None, 4, 128)            1536000   \n_________________________________________________________________\ngru1 (GRU)                   (None, 4, 64)             37248     \n_________________________________________________________________\ngru2 (GRU)                   (None, 4, 32)             9408      \n_________________________________________________________________\ngru3 (GRU)                   (None, 4, 24)             4176      \n_________________________________________________________________\ngru4 (GRU)                   (None, 8)                 816       \n_________________________________________________________________\noutput (Dense)               (None, 1)                 9         \n=================================================================\nTotal params: 1,587,657\nTrainable params: 1,587,657\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","output_type":"stream"}]},{"cell_type":"code","source":"max_epochs = 3\nbatch_size = 128\n\nmodel.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T00:15:38.190193Z","iopub.execute_input":"2022-07-20T00:15:38.190561Z","iopub.status.idle":"2022-07-20T00:16:06.299000Z","shell.execute_reply.started":"2022-07-20T00:15:38.190535Z","shell.execute_reply":"2022-07-20T00:16:06.297489Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"2022-07-20 00:16:06.266287: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at lookup_table_op.cc:929 : Failed precondition: Table not initialized.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_32/1225367912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m:  Table not initialized.\n\t [[node twitter-RNN/text_vectorization_7/string_lookup_7/None_Lookup/LookupTableFindV2 (defined at tmp/ipykernel_32/1225367912.py:4) ]] [Op:__inference_train_function_25022]\n\nFunction call stack:\ntrain_function\n"],"ename":"FailedPreconditionError","evalue":" Table not initialized.\n\t [[node twitter-RNN/text_vectorization_7/string_lookup_7/None_Lookup/LookupTableFindV2 (defined at tmp/ipykernel_32/1225367912.py:4) ]] [Op:__inference_train_function_25022]\n\nFunction call stack:\ntrain_function\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-19T00:20:26.963742Z","iopub.execute_input":"2022-07-19T00:20:26.964482Z","iopub.status.idle":"2022-07-19T00:20:26.970578Z","shell.execute_reply.started":"2022-07-19T00:20:26.964438Z","shell.execute_reply":"2022-07-19T00:20:26.969017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-19T00:20:46.952681Z","iopub.execute_input":"2022-07-19T00:20:46.953058Z","iopub.status.idle":"2022-07-19T00:20:47.378835Z","shell.execute_reply.started":"2022-07-19T00:20:46.953030Z","shell.execute_reply":"2022-07-19T00:20:47.377572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-19T00:21:05.121946Z","iopub.execute_input":"2022-07-19T00:21:05.122473Z","iopub.status.idle":"2022-07-19T00:21:05.499833Z","shell.execute_reply.started":"2022-07-19T00:21:05.122411Z","shell.execute_reply":"2022-07-19T00:21:05.498675Z"},"trusted":true},"execution_count":null,"outputs":[]}]}